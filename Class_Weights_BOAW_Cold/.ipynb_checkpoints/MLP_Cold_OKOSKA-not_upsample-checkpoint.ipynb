{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data mfcc read started...\n",
      "Data mfcc read finished.\n",
      "{'C': 1, 'NC': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17265"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model,Sequential\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report,recall_score,accuracy_score,confusion_matrix,roc_curve,roc_auc_score\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "#Read MFCC data\n",
    "print(\"Data mfcc read started...\")\n",
    "#data = pd.read_csv(\"ComParE2017_Cold.ComParE.train.BoAW.arff\",delimiter=',',skiprows=range(0,2006),header=None)\n",
    "data = pd.read_csv(\"ComParE2017_Cold.ComParE.train.BoAW.upsampled.arff\",delimiter=',',skiprows=range(0,2006),header=None)\n",
    "\n",
    "data=data.as_matrix()\n",
    "print (\"Data mfcc read finished.\")\n",
    "\n",
    "\n",
    "data=data[:,1:2002]\n",
    "\n",
    "\n",
    "Y_train=[x[2000] for x in data]\n",
    "\n",
    "x_train=data[:,0:2000]\n",
    "\n",
    "labels = ['C','NC']\n",
    "label2ind = {label: (index + 1) for index, label in enumerate(labels)}\n",
    "ind2label = {(index + 1): label for index, label in enumerate(labels)}\n",
    "\n",
    "print (label2ind)\n",
    "\n",
    "\n",
    "# In[63]:\n",
    "\n",
    "max_label = max(label2ind.values())+1\n",
    "y_enc = [[label2ind[ey] for ey in Y_train]]\n",
    "\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "def encode(x, n):\n",
    "    result = np.zeros(n)\n",
    "    result[x] = 1\n",
    "    return result\n",
    "\n",
    "y_enc = [[encode(c, max_label) for c in ey] for ey in y_enc]\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "y_enc[0]\n",
    "y_enci =[]\n",
    "for i in (range(len(y_enc))):\n",
    "    a=y_enc[i]\n",
    "    v=np.array(a)\n",
    "    v=np.delete(v, 0, 1)\n",
    "    y_enci.append(v)\n",
    "\n",
    "y_train=y_enci[0]\n",
    "\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data mfcc read started...\n",
      "Data mfcc read finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9596"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read MFCC data\n",
    "print(\"Data mfcc read started...\")\n",
    "data2 = pd.read_csv(\"ComParE2017_Cold.ComParE.devel.BoAW.arff\",delimiter=',',skiprows=range(0,2006),header=None)\n",
    "data2=data2.as_matrix()\n",
    "print (\"Data mfcc read finished.\")\n",
    "\n",
    "\n",
    "data2=data2[:,1:2002]\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "Y_val=[x[2000] for x in data2]\n",
    "\n",
    "x_val=data2[:,0:2000]\n",
    "\n",
    "max_label = max(label2ind.values())+1\n",
    "y_enc = [[label2ind[ey] for ey in Y_val]]\n",
    "y_enc = [[encode(c, max_label) for c in ey] for ey in y_enc]\n",
    "\n",
    "y_enci =[]\n",
    "for i in (range(len(y_enc))):\n",
    "    a=y_enc[i]\n",
    "    v=np.array(a)\n",
    "    v=np.delete(v, 0, 1)\n",
    "    y_enci.append(v)\n",
    "\n",
    "y_val=y_enci[0]\n",
    "\n",
    "len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17265, 2000)\n",
      "(9596, 2000)\n",
      "(9596, 2000)\n",
      "(17265, 2)\n",
      "(9596, 2)\n",
      "(9596, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "\n",
    "\n",
    "train_data_input = scaler.transform(x_train)\n",
    "valid_data_input = scaler.transform(x_val)\n",
    "test_data_input = scaler.transform(x_val)\n",
    "\n",
    "#train_data_input = x_train\n",
    "#valid_data_input = x_val\n",
    "#test_data_input = x_val\n",
    "\n",
    "\n",
    "train_data_target = y_train\n",
    "valid_data_target = y_val\n",
    "test_data_target = y_val\n",
    "# In[80]:\n",
    "print(train_data_input.shape)\n",
    "print(test_data_input.shape)\n",
    "print(valid_data_input.shape)\n",
    "\n",
    "print(train_data_target.shape)\n",
    "print(test_data_target.shape)\n",
    "print(valid_data_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_class_weight(labels_dict):\n",
    "    total = sum(labels_dict.values())\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "\n",
    "    for key in keys:\n",
    "        score = total/float(labels_dict[key])\n",
    "        class_weight[key] = score\n",
    "\n",
    "    return class_weight\n",
    "\n",
    "label_count={}\n",
    "for i in range(train_data_target.shape[-1]):\n",
    "    label_count.update({int(i):len(train_data_target[train_data_target[:,int(i)]==1])})\n",
    "\n",
    "cweights=create_class_weight(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17265 samples, validate on 9596 samples\n",
      "Epoch 1/20\n",
      "17265/17265 [==============================] - 4s - loss: 0.7525 - acc: 0.7896 - val_loss: 0.5060 - val_acc: 0.8507\n",
      "Epoch 2/20\n",
      "17265/17265 [==============================] - 3s - loss: 0.2307 - acc: 0.9166 - val_loss: 0.5918 - val_acc: 0.8437\n",
      "Epoch 3/20\n",
      "17265/17265 [==============================] - 3s - loss: 0.1377 - acc: 0.9531 - val_loss: 0.8001 - val_acc: 0.8508\n",
      "Epoch 4/20\n",
      "17265/17265 [==============================] - 3s - loss: 0.1072 - acc: 0.9655 - val_loss: 0.8650 - val_acc: 0.8594\n",
      "Epoch 5/20\n",
      "17265/17265 [==============================] - 3s - loss: 0.0716 - acc: 0.9775 - val_loss: 1.1397 - val_acc: 0.8588\n",
      "9000/9596 [===========================>..] - ETA: 0sAccuracy:  0.858795330568   Loss:  1.13966769966\n",
      "9312/9596 [============================>.] - ETA: 0s\n",
      "\n",
      "0.594768544372\n"
     ]
    }
   ],
   "source": [
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "# In[87]:\n",
    "\n",
    "model = Sequential()\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, Dropout\n",
    "\n",
    "model.add(Dense(500, input_shape=(2000,), init='glorot_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(500, activation='relu', init='glorot_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(500, activation='relu', init='glorot_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "# In[88]:\n",
    "\n",
    "batch_size=200\n",
    "epochs = 20\n",
    "\n",
    "#sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "model.fit(train_data_input, train_data_target, nb_epoch=epochs,batch_size=batch_size, callbacks=[earlyStopping], shuffle=True, validation_data = (valid_data_input, valid_data_target))#,class_weight=cweights)\n",
    "\n",
    "\n",
    "\n",
    "score = model.evaluate(test_data_input, test_data_target, batch_size=batch_size) #[loss,accuracy]\n",
    "accuracy = score[1]\n",
    "loss = score[0]\n",
    "print(\"Accuracy: \", accuracy, \"  Loss: \", loss)\n",
    "\n",
    "pr = model.predict_classes(test_data_input)\n",
    "yh = test_data_target.argmax(1)\n",
    "print(\"\\n\")\n",
    "print (recall_score(pr, yh, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
