{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data mfcc read started...\n",
      "Data mfcc read finished.\n",
      "{'C': 1, 'NC': 2}\n",
      "Data mfcc read started...\n",
      "Data mfcc read finished.\n",
      "(9504, 6373)\n",
      "(9595, 6373)\n",
      "(9595, 6373)\n",
      "(9504, 2)\n",
      "(9595, 2)\n",
      "(9595, 2)\n",
      "Train on 9504 samples, validate on 9595 samples\n",
      "Epoch 1/20\n",
      "9504/9504 [==============================] - 2s - loss: 1.6536 - acc: 0.8943 - val_loss: 1.6891 - val_acc: 0.8946\n",
      "Epoch 2/20\n",
      "9504/9504 [==============================] - 1s - loss: 1.6340 - acc: 0.8980 - val_loss: 1.6890 - val_acc: 0.8946\n",
      "Epoch 3/20\n",
      "9504/9504 [==============================] - 1s - loss: 1.5868 - acc: 0.8922 - val_loss: 1.6326 - val_acc: 0.8598\n",
      "Epoch 4/20\n",
      "9504/9504 [==============================] - 1s - loss: 1.1633 - acc: 0.8709 - val_loss: 0.6999 - val_acc: 0.8946\n",
      "Epoch 5/20\n",
      "9504/9504 [==============================] - 1s - loss: 0.5167 - acc: 0.8697 - val_loss: 0.3493 - val_acc: 0.8967\n",
      "Epoch 6/20\n",
      "9504/9504 [==============================] - 1s - loss: 0.3559 - acc: 0.8923 - val_loss: 0.4201 - val_acc: 0.8727\n",
      "Epoch 7/20\n",
      "9504/9504 [==============================] - 1s - loss: 0.3194 - acc: 0.8999 - val_loss: 0.3434 - val_acc: 0.8982\n",
      "Epoch 8/20\n",
      "9504/9504 [==============================] - 1s - loss: 0.3034 - acc: 0.9049 - val_loss: 0.3373 - val_acc: 0.8993\n",
      "Epoch 9/20\n",
      "9504/9504 [==============================] - 1s - loss: 0.2899 - acc: 0.9067 - val_loss: 0.3597 - val_acc: 0.8957\n",
      "Epoch 10/20\n",
      "9504/9504 [==============================] - 1s - loss: 0.2857 - acc: 0.9064 - val_loss: 0.3792 - val_acc: 0.8974\n",
      "Epoch 11/20\n",
      "9504/9504 [==============================] - 1s - loss: 0.2863 - acc: 0.9069 - val_loss: 0.4027 - val_acc: 0.8965\n",
      "Epoch 12/20\n",
      "9504/9504 [==============================] - 1s - loss: 0.2836 - acc: 0.9067 - val_loss: 0.3980 - val_acc: 0.8915\n",
      "9595/9595 [==============================] - 0s     \n",
      "Accuracy:  0.891505991344   Loss:  0.398041623856\n",
      "9152/9595 [===========================>..] - ETA: 0s\n",
      "\n",
      "0.674939720919\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model,Sequential\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report,recall_score,accuracy_score,confusion_matrix,roc_curve,roc_auc_score\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "#Read MFCC data\n",
    "print(\"Data mfcc read started...\")\n",
    "data = pd.read_csv(\"ComParE2017_Cold.ComParE.train__2.arff\",delimiter=',')\n",
    "data=data.as_matrix()\n",
    "print (\"Data mfcc read finished.\")\n",
    "\n",
    "\n",
    "data=data[:,1:6375]\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "Y_train=[x[6373] for x in data]\n",
    "Y_train[300]\n",
    "\n",
    "x_train=data[:,0:6373]\n",
    "\n",
    "labels = ['C','NC']\n",
    "label2ind = {label: (index + 1) for index, label in enumerate(labels)}\n",
    "ind2label = {(index + 1): label for index, label in enumerate(labels)}\n",
    "\n",
    "print (label2ind)\n",
    "\n",
    "\n",
    "# In[63]:\n",
    "\n",
    "max_label = max(label2ind.values())+1\n",
    "y_enc = [[label2ind[ey] for ey in Y_train]]\n",
    "\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "def encode(x, n):\n",
    "    result = np.zeros(n)\n",
    "    result[x] = 1\n",
    "    return result\n",
    "\n",
    "y_enc = [[encode(c, max_label) for c in ey] for ey in y_enc]\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "y_enc[0]\n",
    "y_enci =[]\n",
    "for i in (range(len(y_enc))):\n",
    "    a=y_enc[i]\n",
    "    v=np.array(a)\n",
    "    v=np.delete(v, 0, 1)\n",
    "    y_enci.append(v)\n",
    "\n",
    "y_train=y_enci[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Read MFCC data\n",
    "print(\"Data mfcc read started...\")\n",
    "data2 = pd.read_csv(\"ComParE2017_Cold.ComParE.devel___2.arff\",delimiter=',')\n",
    "data2=data2.as_matrix()\n",
    "print (\"Data mfcc read finished.\")\n",
    "\n",
    "\n",
    "data2=data2[:,1:6375]\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "Y_val=[x[6373] for x in data2]\n",
    "x_val=data2[:,0:6373]\n",
    "\n",
    "max_label = max(label2ind.values())+1\n",
    "y_enc = [[label2ind[ey] for ey in Y_val]]\n",
    "y_enc = [[encode(c, max_label) for c in ey] for ey in y_enc]\n",
    "\n",
    "y_enci =[]\n",
    "for i in (range(len(y_enc))):\n",
    "    a=y_enc[i]\n",
    "    v=np.array(a)\n",
    "    v=np.delete(v, 0, 1)\n",
    "    y_enci.append(v)\n",
    "\n",
    "y_val=y_enci[0]\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "\n",
    "\n",
    "train_data_input = scaler.transform(x_train)\n",
    "valid_data_input = scaler.transform(x_val)\n",
    "test_data_input = scaler.transform(x_val)\n",
    "\n",
    "#train_data_input = x_train\n",
    "#valid_data_input = x_val\n",
    "#test_data_input = x_val\n",
    "\n",
    "\n",
    "train_data_target = y_train\n",
    "valid_data_target = y_val\n",
    "test_data_target = y_val\n",
    "# In[80]:\n",
    "print(train_data_input.shape)\n",
    "print(test_data_input.shape)\n",
    "print(valid_data_input.shape)\n",
    "\n",
    "print(train_data_target.shape)\n",
    "print(test_data_target.shape)\n",
    "print(valid_data_target.shape)\n",
    "\n",
    "#train_data_input = train_data_input[:,0:6373:2]\n",
    "#valid_data_input = valid_data_input[:,0:6373:2]\n",
    "#test_data_input = test_data_input[:,0:6373:2]\n",
    "\n",
    "#print(train_data_input.shape)\n",
    "#print(test_data_input.shape)\n",
    "#print(valid_data_input.shape)\n",
    "\n",
    "#print(train_data_target.shape)\n",
    "#print(test_data_target.shape)\n",
    "#print(valid_data_target.shape)\n",
    "\n",
    "\n",
    "\n",
    "# on non-standardized data\n",
    "pca = PCA(n_components=20).fit(train_data_input)\n",
    "train_data_input = pca.transform(train_data_input)\n",
    "valid_data_input = pca.transform(valid_data_input)\n",
    "test_data_input = pca.transform(test_data_input)\n",
    "\n",
    "\n",
    "# In[236]:\n",
    "\n",
    "def create_class_weight(labels_dict):\n",
    "    total = sum(labels_dict.values())\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "\n",
    "    for key in keys:\n",
    "        score = total/float(labels_dict[key])\n",
    "        class_weight[key] = score\n",
    "\n",
    "    return class_weight\n",
    "\n",
    "label_count={}\n",
    "for i in range(train_data_target.shape[-1]):\n",
    "    label_count.update({int(i):len(train_data_target[train_data_target[:,int(i)]==1])})\n",
    "\n",
    "cweights=create_class_weight(label_count)\n",
    "\n",
    "\n",
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "# In[87]:\n",
    "\n",
    "model = Sequential()\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, Dropout\n",
    "\n",
    "model.add(Dense(500, input_shape=(20,), init='glorot_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(500, activation='relu', init='glorot_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(500, activation='relu', init='glorot_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "# In[88]:\n",
    "\n",
    "batch_size=100\n",
    "epochs = 20\n",
    "\n",
    "#sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "model.fit(train_data_input, train_data_target, nb_epoch=epochs,batch_size=batch_size, callbacks=[earlyStopping], shuffle=True, validation_data = (valid_data_input, valid_data_target))#class_weight=cweights)\n",
    "\n",
    "\n",
    "\n",
    "score = model.evaluate(test_data_input, test_data_target, batch_size=batch_size) #[loss,accuracy]\n",
    "accuracy = score[1]\n",
    "loss = score[0]\n",
    "print(\"Accuracy: \", accuracy, \"  Loss: \", loss)\n",
    "\n",
    "pr = model.predict_classes(test_data_input)\n",
    "yh = test_data_target.argmax(1)\n",
    "print(\"\\n\")\n",
    "print (recall_score(pr, yh, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
